{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "neuralcollapse.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neuralcollapse/neuralcollapse/blob/main/neuralcollapse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thUHK_rfnXaP"
      },
      "source": [
        "# Neural Collapse Examples\n",
        "\n",
        "Code demonstrating Neural Collapse on Cross-Entropy [1] and MSE Loss [2].\n",
        "Notebook is designed to be short, easy-to-interpret, and executable\n",
        "from the browser using Google Colab.\n",
        "\n",
        "MNIST-ResNet18 was chosen because it ran most reliably within the in-browser\n",
        "memory constraints of Google Colab.\n",
        "If you are *still* getting out-of-memory errors, try clicking\n",
        "\"Runtime\"->\"Factory Reset Runtime\" on the menu bar.\n",
        "\n",
        "It should be clear how to adapt the code to other networks-dataset combinations\n",
        "to be run on local clusters with more memory.\n",
        "\n",
        "### References:\n",
        "\n",
        "[1] V. Papyan, X.Y. Han, and D.L. Donoho. \"[Prevalence of Neural Collapse During the Terminal Phase of Deep Learning Training.](https://www.pnas.org/content/117/40/24652)\" *Proceedings of the National Academy of Sciences (PNAS)* 117, no. 40 (2020): 24652-24663.\n",
        "\n",
        "[2] X.Y. Han, V. Papyan, and D.L. Donoho. [“Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path.”](https://openreview.net/forum?id=w1UbdvWH_R3) In *International Conference on Learning Representations (ICLR)*, 2022.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C4d902hIh43"
      },
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import gc\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "\n",
        "from tqdm import tqdm\n",
        "from collections import OrderedDict\n",
        "from scipy.sparse.linalg import svds\n",
        "from torchvision import datasets, transforms\n",
        "from IPython import embed\n",
        "\n",
        "debug = True # Only runs 20 batches per epoch for debugging\n",
        "\n",
        "# dataset parameters\n",
        "im_size             = 28\n",
        "padded_im_size      = 32\n",
        "C                   = 10\n",
        "input_ch            = 1\n",
        "\n",
        "# Optimization Criterion\n",
        "# loss_name = 'CrossEntropyLoss'\n",
        "loss_name = 'MSELoss'\n",
        "\n",
        "# Optimization hyperparameters\n",
        "lr_decay            = 0.1\n",
        "\n",
        "# Best lr after hyperparameter tuning\n",
        "if loss_name == 'CrossEntropyLoss':\n",
        "  lr = 0.0679\n",
        "elif loss_name == 'MSELoss':\n",
        "  lr = 0.0184\n",
        "\n",
        "epochs              = 350\n",
        "epochs_lr_decay     = [epochs//3, epochs*2//3]\n",
        "\n",
        "batch_size          = 128\n",
        "\n",
        "momentum            = 0.9\n",
        "weight_decay        = 5e-4\n",
        "\n",
        "# analysis parameters\n",
        "epoch_list          = [1,   2,   3,   4,   5,   6,   7,   8,   9,   10,   11,\n",
        "                       12,  13,  14,  16,  17,  19,  20,  22,  24,  27,   29,\n",
        "                       32,  35,  38,  42,  45,  50,  54,  59,  65,  71,   77,\n",
        "                       85,  92,  101, 110, 121, 132, 144, 158, 172, 188,  206,\n",
        "                       225, 245, 268, 293, 320, 350]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IODk-OmCIm8H"
      },
      "source": [
        "def train(model, criterion, device, num_classes, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    \n",
        "    pbar = tqdm(total=len(train_loader), position=0, leave=True)\n",
        "    for batch_idx, (data, target) in enumerate(train_loader, start=1):\n",
        "        if data.shape[0] != batch_size:\n",
        "            continue\n",
        "        \n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        if str(criterion) == 'CrossEntropyLoss()':\n",
        "          loss = criterion(out, target)\n",
        "        elif str(criterion) == 'MSELoss()':\n",
        "          loss = criterion(out, F.one_hot(target, num_classes=num_classes).float()) \n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        accuracy = torch.mean((torch.argmax(out,dim=1)==target).float()).item()\n",
        "\n",
        "        pbar.update(1)\n",
        "        pbar.set_description(\n",
        "            'Train\\t\\tEpoch: {} [{}/{} ({:.0f}%)] \\t'\n",
        "            'Batch Loss: {:.6f} \\t'\n",
        "            'Batch Accuracy: {:.6f}'.format(\n",
        "                epoch,\n",
        "                batch_idx,\n",
        "                len(train_loader),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "                loss.item(),\n",
        "                accuracy))\n",
        "        \n",
        "        if debug and batch_idx > 20:\n",
        "          break\n",
        "    pbar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "vtHzbdibIo6J"
      },
      "source": [
        "def analysis(graphs, model, criterion_summed, device, num_classes, loader):\n",
        "    model.eval()\n",
        "\n",
        "    N             = [0 for _ in range(C)]\n",
        "    mean          = [0 for _ in range(C)]\n",
        "    Sw            = 0\n",
        "\n",
        "    loss          = 0\n",
        "    net_correct   = 0\n",
        "    NCC_match_net = 0\n",
        "\n",
        "    for computation in ['Mean','Cov']:\n",
        "        pbar = tqdm(total=len(loader), position=0, leave=True)\n",
        "        for batch_idx, (data, target) in enumerate(loader, start=1):\n",
        "\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            output = model(data)\n",
        "            h = features.value.data.view(data.shape[0],-1) # B CHW\n",
        "            \n",
        "            # during calculation of class means, calculate loss\n",
        "            if computation == 'Mean':\n",
        "                if str(criterion_summed) == 'CrossEntropyLoss()':\n",
        "                  loss += criterion_summed(output, target).item()\n",
        "                elif str(criterion_summed) == 'MSELoss()':\n",
        "                  loss += criterion_summed(output, F.one_hot(target, num_classes=num_classes).float()).item()\n",
        "\n",
        "            for c in range(C):\n",
        "                # features belonging to class c\n",
        "                idxs = (target == c).nonzero(as_tuple=True)[0]\n",
        "                \n",
        "                if len(idxs) == 0: # If no class-c in this batch\n",
        "                  continue\n",
        "\n",
        "                h_c = h[idxs,:] # B CHW\n",
        "\n",
        "                if computation == 'Mean':\n",
        "                    # update class means\n",
        "                    mean[c] += torch.sum(h_c, dim=0) # CHW\n",
        "                    N[c] += h_c.shape[0]\n",
        "                    \n",
        "                elif computation == 'Cov':\n",
        "                    # update within-class cov\n",
        "\n",
        "                    z = h_c - mean[c].unsqueeze(0) # B CHW\n",
        "                    cov = torch.matmul(z.unsqueeze(-1), # B CHW 1\n",
        "                                       z.unsqueeze(1))  # B 1 CHW\n",
        "                    Sw += torch.sum(cov, dim=0)\n",
        "\n",
        "                    # during calculation of within-class covariance, calculate:\n",
        "                    # 1) network's accuracy\n",
        "                    net_pred = torch.argmax(output[idxs,:], dim=1)\n",
        "                    net_correct += sum(net_pred==target[idxs]).item()\n",
        "\n",
        "                    # 2) agreement between prediction and nearest class center\n",
        "                    NCC_scores = torch.stack([torch.norm(h_c[i,:] - M.T,dim=1) \\\n",
        "                                              for i in range(h_c.shape[0])])\n",
        "                    NCC_pred = torch.argmin(NCC_scores, dim=1)\n",
        "                    NCC_match_net += sum(NCC_pred==net_pred).item()\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_description(\n",
        "                'Analysis {}\\t'\n",
        "                'Epoch: {} [{}/{} ({:.0f}%)]'.format(\n",
        "                    computation,\n",
        "                    epoch,\n",
        "                    batch_idx,\n",
        "                    len(loader),\n",
        "                    100. * batch_idx/ len(loader)))\n",
        "            \n",
        "            if debug and batch_idx > 20:\n",
        "                break\n",
        "        pbar.close()\n",
        "        \n",
        "        if computation == 'Mean':\n",
        "            for c in range(C):\n",
        "                mean[c] /= N[c]\n",
        "                M = torch.stack(mean).T\n",
        "            loss /= sum(N)\n",
        "        elif computation == 'Cov':\n",
        "            Sw /= sum(N)\n",
        "    \n",
        "    graphs.loss.append(loss)\n",
        "    graphs.accuracy.append(net_correct/sum(N))\n",
        "    graphs.NCC_mismatch.append(1-NCC_match_net/sum(N))\n",
        "\n",
        "    # loss with weight decay\n",
        "    reg_loss = loss\n",
        "    for param in model.parameters():\n",
        "        reg_loss += 0.5 * weight_decay * torch.sum(param**2).item()\n",
        "    graphs.reg_loss.append(reg_loss)\n",
        "\n",
        "    # global mean\n",
        "    muG = torch.mean(M, dim=1, keepdim=True) # CHW 1\n",
        "    \n",
        "    # between-class covariance\n",
        "    M_ = M - muG\n",
        "    Sb = torch.matmul(M_, M_.T) / C\n",
        "\n",
        "    # avg norm\n",
        "    W  = classifier.weight\n",
        "    M_norms = torch.norm(M_,  dim=0)\n",
        "    W_norms = torch.norm(W.T, dim=0)\n",
        "\n",
        "    graphs.norm_M_CoV.append((torch.std(M_norms)/torch.mean(M_norms)).item())\n",
        "    graphs.norm_W_CoV.append((torch.std(W_norms)/torch.mean(W_norms)).item())\n",
        "\n",
        "    # Decomposition of MSE #\n",
        "    if loss_name == 'MSELoss':\n",
        "\n",
        "      wd = 0.5 * weight_decay # \"\\lambda\" in manuscript, so this is halved\n",
        "      St = Sw+Sb\n",
        "      size_last_layer = Sb.shape[0]\n",
        "      eye_P = torch.eye(size_last_layer).to(device)\n",
        "      eye_C = torch.eye(C).to(device)\n",
        "\n",
        "      St_inv = torch.inverse(St + (wd/(wd+1))*(muG @ muG.T) + wd*eye_P)\n",
        "\n",
        "      w_LS = 1 / C * (M.T - 1 / (1 + wd) * muG.T) @ St_inv\n",
        "      b_LS = (1/C * torch.ones(C).to(device) - w_LS @ muG.T.squeeze(0)) / (1+wd)\n",
        "      w_LS_ = torch.cat([w_LS, b_LS.unsqueeze(-1)], dim=1)  # c x n\n",
        "      b  = classifier.bias\n",
        "      w_ = torch.cat([W, b.unsqueeze(-1)], dim=1)  # c x n\n",
        "\n",
        "      LNC1 = 0.5 * (torch.trace(w_LS @ (Sw + wd*eye_P) @ w_LS.T) + wd*torch.norm(b_LS)**2)\n",
        "      LNC23 = 0.5/C * torch.norm(w_LS @ M + b_LS.unsqueeze(1) - eye_C) ** 2\n",
        "\n",
        "      A1 = torch.cat([St + muG @ muG.T + wd*eye_P, muG], dim=1)\n",
        "      A2 = torch.cat([muG.T, torch.ones([1,1]).to(device) + wd], dim=1)\n",
        "      A = torch.cat([A1, A2], dim=0)\n",
        "      Lperp = 0.5 * torch.trace((w_ - w_LS_) @ A @ (w_ - w_LS_).T)\n",
        "\n",
        "      MSE_wd_features = loss + 0.5* weight_decay * (torch.norm(W)**2 + torch.norm(b)**2).item()\n",
        "      MSE_wd_features *= 0.5\n",
        "\n",
        "      graphs.MSE_wd_features.append(MSE_wd_features)\n",
        "      graphs.LNC1.append(LNC1.item())\n",
        "      graphs.LNC23.append(LNC23.item())\n",
        "      graphs.Lperp.append(Lperp.item())\n",
        "\n",
        "    # tr{Sw Sb^-1}\n",
        "    Sw = Sw.cpu().numpy()\n",
        "    Sb = Sb.cpu().numpy()\n",
        "    eigvec, eigval, _ = svds(Sb, k=C-1)\n",
        "    inv_Sb = eigvec @ np.diag(eigval**(-1)) @ eigvec.T \n",
        "    graphs.Sw_invSb.append(np.trace(Sw @ inv_Sb))\n",
        "\n",
        "    # ||W^T - M_||\n",
        "    normalized_M = M_ / torch.norm(M_,'fro')\n",
        "    normalized_W = W.T / torch.norm(W.T,'fro')\n",
        "    graphs.W_M_dist.append((torch.norm(normalized_W - normalized_M)**2).item())\n",
        "\n",
        "    # mutual coherence\n",
        "    def coherence(V): \n",
        "        G = V.T @ V\n",
        "        G += torch.ones((C,C),device=device) / (C-1)\n",
        "        G -= torch.diag(torch.diag(G))\n",
        "        return torch.norm(G,1).item() / (C*(C-1))\n",
        "\n",
        "    graphs.cos_M.append(coherence(M_/M_norms))\n",
        "    graphs.cos_W.append(coherence(W.T/W_norms))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDbY5HycIrse"
      },
      "source": [
        "model = models.resnet18(pretrained=False, num_classes=C)\n",
        "model.conv1 = nn.Conv2d(input_ch, model.conv1.weight.shape[0], 3, 1, 1, bias=False) # Small dataset filter size used by He et al. (2015)\n",
        "model.maxpool = nn.MaxPool2d(kernel_size=1, stride=1, padding=0)\n",
        "model = model.to(device)\n",
        "\n",
        "class features:\n",
        "    pass\n",
        "\n",
        "def hook(self, input, output):\n",
        "    features.value = input[0].clone()\n",
        "\n",
        "# register hook that saves last-layer input into features\n",
        "classifier = model.fc\n",
        "classifier.register_forward_hook(hook)\n",
        "\n",
        "transform = transforms.Compose([transforms.Pad((padded_im_size - im_size)//2),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(0.1307,0.3081)])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "\n",
        "analysis_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "\n",
        "if loss_name == 'CrossEntropyLoss':\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  criterion_summed = nn.CrossEntropyLoss(reduction='sum')\n",
        "elif loss_name == 'MSELoss':\n",
        "  criterion = nn.MSELoss()\n",
        "  criterion_summed = nn.MSELoss(reduction='sum')\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(),\n",
        "                      lr=lr,\n",
        "                      momentum=momentum,\n",
        "                      weight_decay=weight_decay)\n",
        "\n",
        "lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                              milestones=epochs_lr_decay,\n",
        "                                              gamma=lr_decay)\n",
        "\n",
        "class graphs:\n",
        "  def __init__(self):\n",
        "    self.accuracy     = []\n",
        "    self.loss         = []\n",
        "    self.reg_loss     = []\n",
        "\n",
        "    # NC1\n",
        "    self.Sw_invSb     = []\n",
        "\n",
        "    # NC2\n",
        "    self.norm_M_CoV   = []\n",
        "    self.norm_W_CoV   = []\n",
        "    self.cos_M        = []\n",
        "    self.cos_W        = []\n",
        "\n",
        "    # NC3\n",
        "    self.W_M_dist     = []\n",
        "    \n",
        "    # NC4\n",
        "    self.NCC_mismatch = []\n",
        "\n",
        "    # Decomposition\n",
        "    self.MSE_wd_features = []\n",
        "    self.LNC1 = []\n",
        "    self.LNC23 = []\n",
        "    self.Lperp = []\n",
        "graphs = graphs()\n",
        "\n",
        "cur_epochs = []\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, criterion, device, C, train_loader, optimizer, epoch)\n",
        "    lr_scheduler.step()\n",
        "    \n",
        "    if epoch in epoch_list:\n",
        "        cur_epochs.append(epoch)\n",
        "        analysis(graphs, model, criterion_summed, device, C, analysis_loader)\n",
        "        \n",
        "        plt.figure(1)\n",
        "        plt.semilogy(cur_epochs, graphs.reg_loss)\n",
        "        plt.legend(['Loss + Weight Decay'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Value')\n",
        "        plt.title('Training Loss')\n",
        "\n",
        "        plt.figure(2)\n",
        "        plt.plot(cur_epochs, 100*(1 - np.array(graphs.accuracy)))\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Training Error (%)')\n",
        "        plt.title('Training Error')\n",
        "\n",
        "        plt.figure(3)\n",
        "        plt.semilogy(cur_epochs, graphs.Sw_invSb)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Tr{Sw Sb^-1}')\n",
        "        plt.title('NC1: Activation Collapse')\n",
        "\n",
        "        plt.figure(4)\n",
        "        plt.plot(cur_epochs, graphs.norm_M_CoV)\n",
        "        plt.plot(cur_epochs, graphs.norm_W_CoV)\n",
        "        plt.legend(['Class Means','Classifiers'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Std/Avg of Norms')\n",
        "        plt.title('NC2: Equinorm')\n",
        "        \n",
        "        plt.figure(5)\n",
        "        plt.plot(cur_epochs, graphs.cos_M)\n",
        "        plt.plot(cur_epochs, graphs.cos_W)\n",
        "        plt.legend(['Class Means','Classifiers'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Avg|Cos + 1/(C-1)|')\n",
        "        plt.title('NC2: Maximal Equiangularity')\n",
        "\n",
        "        plt.figure(6)\n",
        "        plt.plot(cur_epochs,graphs.W_M_dist)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('||W^T - H||^2')\n",
        "        plt.title('NC3: Self Duality')\n",
        "\n",
        "        plt.figure(7)\n",
        "        plt.plot(cur_epochs,graphs.NCC_mismatch)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Proportion Mismatch from NCC')\n",
        "        plt.title('NC4: Convergence to NCC')\n",
        "\n",
        "        # Plot decomposition of MSE loss\n",
        "        if loss_name == 'MSELoss':\n",
        "          plt.figure(8)\n",
        "          plt.semilogy(cur_epochs, graphs.MSE_wd_features)\n",
        "          plt.semilogy(cur_epochs, graphs.LNC1)\n",
        "          plt.semilogy(cur_epochs, graphs.LNC23)\n",
        "          plt.semilogy(cur_epochs, graphs.Lperp)\n",
        "          plt.legend(['MSE+wd', 'LNC1', 'LNC2/3', 'Lperp'])\n",
        "          plt.xlabel('Epoch')\n",
        "          plt.ylabel('Value')\n",
        "          plt.title('Decomposition of MSE')\n",
        "\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYOEL_46KlOI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}