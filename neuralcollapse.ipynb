{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "neuralcollapse.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neuralcollapse/neuralcollapse/blob/main/neuralcollapse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C4d902hIh43"
      },
      "source": [
        "# Code demonstrating Neural Collapse on Cross-Entropy and MSE Loss.\n",
        "# Notebook is designed to be short, easy-to-interpret, and executable\n",
        "# from the browser using Google Colab.\n",
        "\n",
        "# MNIST-ResNet18 was chosen because it ran most reliably within the in-browser\n",
        "# the memory constraints of Google Colab.\n",
        "# If you are *still* getting out-of-memory errors, try clicking\n",
        "# \"Runtime\"->\"Factory Reset Runtime\" on the menu bar.\n",
        "\n",
        "# It should be clear how to adapt code to other networks-dataset combinations\n",
        "# to be run on local clusters with more memory, but if you run into trouble,\n",
        "# please feel free to contact the authors. We'd be glad to help.\n",
        "# Happy experimenting!\n",
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "import gc\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from collections import OrderedDict\n",
        "\n",
        "from tqdm import tqdm\n",
        "from scipy.sparse.linalg import svds\n",
        "from torchvision import datasets, transforms\n",
        "from IPython import embed\n",
        "\n",
        "debug = False # Only runs 20 batches per epoch for debugging\n",
        "\n",
        "# dataset parameters\n",
        "im_size             = 28\n",
        "padded_im_size      = 32\n",
        "C                   = 10\n",
        "input_ch            = 1\n",
        "\n",
        "# Optimization Criterion\n",
        "# loss_name = 'CrossEntropyLoss'\n",
        "loss_name = 'MSELoss'\n",
        "\n",
        "# Optimization hyperparameters\n",
        "lr_decay            = 0.1\n",
        "\n",
        "# Best lr after hyperparameter tuning\n",
        "if loss_name == 'CrossEntropyLoss':\n",
        "  lr = 0.0679\n",
        "elif loss_name == 'MSELoss':\n",
        "  lr = 0.0184\n",
        "\n",
        "epochs              = 350\n",
        "epochs_lr_decay     = [epochs//3, epochs*2//3]\n",
        "\n",
        "batch_size          = 128\n",
        "\n",
        "momentum            = 0.9\n",
        "weight_decay        = 5e-4\n",
        "\n",
        "# analysis parameters\n",
        "epoch_list          = [1,   2,   3,   4,   5,   6,   7,   8,   9,   10,   11,\n",
        "                       12,  13,  14,  16,  17,  19,  20,  22,  24,  27,   29,\n",
        "                       32,  35,  38,  42,  45,  50,  54,  59,  65,  71,   77,\n",
        "                       85,  92,  101, 110, 121, 132, 144, 158, 172, 188,  206,\n",
        "                       225, 245, 268, 293, 320, 350]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IODk-OmCIm8H"
      },
      "source": [
        "def train(model, criterion, device, num_classes, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    \n",
        "    pbar = tqdm(total=len(train_loader), position=0, leave=True)\n",
        "    for batch_idx, (data, target) in enumerate(train_loader, start=1):\n",
        "        if data.shape[0] != batch_size:\n",
        "            continue\n",
        "        \n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        if str(criterion) == 'CrossEntropyLoss()':\n",
        "          loss = criterion(out, target)\n",
        "        elif str(criterion) == 'MSELoss()':\n",
        "          loss = criterion(out, F.one_hot(target, num_classes=num_classes).float()) \n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        accuracy = torch.mean((torch.argmax(out,dim=1)==target).float()).item()\n",
        "\n",
        "        pbar.update(1)\n",
        "        pbar.set_description(\n",
        "            'Train\\t\\tEpoch: {} [{}/{} ({:.0f}%)] \\t'\n",
        "            'Batch Loss: {:.6f} \\t'\n",
        "            'Batch Accuracy: {:.6f}'.format(\n",
        "                epoch,\n",
        "                batch_idx,\n",
        "                len(train_loader),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "                loss.item(),\n",
        "                accuracy))\n",
        "        \n",
        "        if debug and batch_idx > 20:\n",
        "          break\n",
        "    pbar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "vtHzbdibIo6J"
      },
      "source": [
        "def analysis(graphs, model, criterion_summed, device, num_classes, loader):\n",
        "    model.eval()\n",
        "\n",
        "    N             = [0 for _ in range(C)]\n",
        "    mean          = [0 for _ in range(C)]\n",
        "    Sw            = 0\n",
        "\n",
        "    loss          = 0\n",
        "    net_correct   = 0\n",
        "    NCC_match_net = 0\n",
        "\n",
        "    for computation in ['Mean','Cov']:\n",
        "        pbar = tqdm(total=len(loader), position=0, leave=True)\n",
        "        for batch_idx, (data, target) in enumerate(loader, start=1):\n",
        "\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            output = model(data)\n",
        "            h = features.value.data.view(data.shape[0],-1) # B CHW\n",
        "            \n",
        "            # during calculation of class means, calculate loss\n",
        "            if computation == 'Mean':\n",
        "                if str(criterion_summed) == 'CrossEntropyLoss()':\n",
        "                  loss += criterion_summed(output, target).item()\n",
        "                elif str(criterion_summed) == 'MSELoss()':\n",
        "                  loss += criterion_summed(output, F.one_hot(target, num_classes=num_classes).float()).item()\n",
        "\n",
        "            for c in range(C):\n",
        "                # features belonging to class c\n",
        "                idxs = (target == c).nonzero(as_tuple=True)[0]\n",
        "                \n",
        "                if len(idxs) == 0: # If no class-c in this batch\n",
        "                  continue\n",
        "\n",
        "                h_c = h[idxs,:] # B CHW\n",
        "\n",
        "                if computation == 'Mean':\n",
        "                    # update class means\n",
        "                    mean[c] += torch.sum(h_c, dim=0) #Â CHW\n",
        "                    N[c] += h_c.shape[0]\n",
        "                    \n",
        "                elif computation == 'Cov':\n",
        "                    # update within-class cov\n",
        "\n",
        "                    z = h_c - mean[c].unsqueeze(0) # B CHW\n",
        "                    cov = torch.matmul(z.unsqueeze(-1), # B CHW 1\n",
        "                                       z.unsqueeze(1))  # B 1 CHW\n",
        "                    Sw += torch.sum(cov, dim=0)\n",
        "\n",
        "                    # during calculation of within-class covariance, calculate:\n",
        "                    # 1) network's accuracy\n",
        "                    net_pred = torch.argmax(output[idxs,:], dim=1)\n",
        "                    net_correct += sum(net_pred==target[idxs]).item()\n",
        "\n",
        "                    # 2) agreement between prediction and nearest class center\n",
        "                    NCC_scores = torch.stack([torch.norm(h_c[i,:] - M.T,dim=1) \\\n",
        "                                              for i in range(h_c.shape[0])])\n",
        "                    NCC_pred = torch.argmin(NCC_scores, dim=1)\n",
        "                    NCC_match_net += sum(NCC_pred==net_pred).item()\n",
        "\n",
        "            pbar.update(1)\n",
        "            pbar.set_description(\n",
        "                'Analysis {}\\t'\n",
        "                'Epoch: {} [{}/{} ({:.0f}%)]'.format(\n",
        "                    computation,\n",
        "                    epoch,\n",
        "                    batch_idx,\n",
        "                    len(loader),\n",
        "                    100. * batch_idx/ len(loader)))\n",
        "            \n",
        "            if debug and batch_idx > 20:\n",
        "                break\n",
        "        pbar.close()\n",
        "        \n",
        "        if computation == 'Mean':\n",
        "            for c in range(C):\n",
        "                mean[c] /= N[c]\n",
        "                M = torch.stack(mean).T\n",
        "            loss /= sum(N)\n",
        "        elif computation == 'Cov':\n",
        "            Sw /= sum(N)\n",
        "    \n",
        "    graphs.loss.append(loss)\n",
        "    graphs.accuracy.append(net_correct/sum(N))\n",
        "    graphs.NCC_mismatch.append(1-NCC_match_net/sum(N))\n",
        "\n",
        "    # loss with weight decay\n",
        "    reg_loss = loss\n",
        "    for param in model.parameters():\n",
        "        reg_loss += 0.5 * weight_decay * torch.sum(param**2).item()\n",
        "    graphs.reg_loss.append(reg_loss)\n",
        "\n",
        "    # global mean\n",
        "    muG = torch.mean(M, dim=1, keepdim=True) # CHW 1\n",
        "    \n",
        "    # between-class covariance\n",
        "    M_ = M - muG\n",
        "    Sb = torch.matmul(M_, M_.T) / C\n",
        "\n",
        "    # tr{Sw Sb^-1}\n",
        "    Sw = Sw.cpu().numpy()\n",
        "    Sb = Sb.cpu().numpy()\n",
        "    eigvec, eigval, _ = svds(Sb, k=C-1)\n",
        "    inv_Sb = eigvec @ np.diag(eigval**(-1)) @ eigvec.T \n",
        "    graphs.Sw_invSb.append(np.trace(Sw @ inv_Sb))\n",
        "\n",
        "    # avg norm\n",
        "    W  = classifier.weight\n",
        "    M_norms = torch.norm(M_,  dim=0)\n",
        "    W_norms = torch.norm(W.T, dim=0)\n",
        "\n",
        "    graphs.norm_M_CoV.append((torch.std(M_norms)/torch.mean(M_norms)).item())\n",
        "    graphs.norm_W_CoV.append((torch.std(W_norms)/torch.mean(W_norms)).item())\n",
        "\n",
        "    # ||W^T - M_||\n",
        "    normalized_M = M_ / torch.norm(M_,'fro')\n",
        "    normalized_W = W.T / torch.norm(W.T,'fro')\n",
        "    graphs.W_M_dist.append((torch.norm(normalized_W - normalized_M)**2).item())\n",
        "\n",
        "    # mutual coherence\n",
        "    def coherence(V): \n",
        "        G = V.T @ V\n",
        "        G += torch.ones((C,C),device=device) / (C-1)\n",
        "        G -= torch.diag(torch.diag(G))\n",
        "        return torch.norm(G,1).item() / (C*(C-1))\n",
        "\n",
        "    graphs.cos_M.append(coherence(M_/M_norms))\n",
        "    graphs.cos_W.append(coherence(W.T/W_norms))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDbY5HycIrse"
      },
      "source": [
        "model = models.resnet18(pretrained=False, num_classes=C)\n",
        "model.conv1 = nn.Conv2d(input_ch, model.conv1.weight.shape[0], 3, 1, 1, bias=False) # Small dataset filter size used by He et al. (2015)\n",
        "model.maxpool = nn.MaxPool2d(kernel_size=1, stride=1, padding=0)\n",
        "model = model.to(device)\n",
        "\n",
        "class features:\n",
        "    pass\n",
        "\n",
        "def hook(self, input, output):\n",
        "    features.value = input[0].clone()\n",
        "\n",
        "# register hook that saves last-layer input into features\n",
        "classifier = model.fc\n",
        "classifier.register_forward_hook(hook)\n",
        "\n",
        "transform = transforms.Compose([transforms.Pad((padded_im_size - im_size)//2),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(0.1307,0.3081)])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "\n",
        "analysis_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "\n",
        "if loss_name == 'CrossEntropyLoss':\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  criterion_summed = nn.CrossEntropyLoss(reduction='sum')\n",
        "elif loss_name == 'MSELoss':\n",
        "  criterion = nn.MSELoss()\n",
        "  criterion_summed = nn.MSELoss(reduction='sum')\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(),\n",
        "                      lr=lr,\n",
        "                      momentum=momentum,\n",
        "                      weight_decay=weight_decay)\n",
        "\n",
        "lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                              milestones=epochs_lr_decay,\n",
        "                                              gamma=lr_decay)\n",
        "\n",
        "class graphs:\n",
        "  def __init__(self):\n",
        "    self.accuracy     = []\n",
        "    self.loss         = []\n",
        "    self.reg_loss     = []\n",
        "\n",
        "    # NC1\n",
        "    self.Sw_invSb     = []\n",
        "\n",
        "    # NC2\n",
        "    self.norm_M_CoV   = []\n",
        "    self.norm_W_CoV   = []\n",
        "    self.cos_M        = []\n",
        "    self.cos_W        = []\n",
        "\n",
        "    # NC3\n",
        "    self.W_M_dist     = []\n",
        "    \n",
        "    # NC4\n",
        "    self.NCC_mismatch = []\n",
        "graphs = graphs()\n",
        "\n",
        "cur_epochs = []\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, criterion, device, C, train_loader, optimizer, epoch)\n",
        "    lr_scheduler.step()\n",
        "    \n",
        "    if epoch in epoch_list:\n",
        "        cur_epochs.append(epoch)\n",
        "        analysis(graphs, model, criterion_summed, device, C, analysis_loader)\n",
        "        \n",
        "        plt.figure(1)\n",
        "        plt.semilogy(cur_epochs, graphs.reg_loss)\n",
        "        plt.legend(['Loss + Weight Decay'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Value')\n",
        "        plt.title('Training Loss')\n",
        "\n",
        "        plt.figure(2)\n",
        "        plt.plot(cur_epochs, 100*(1 - np.array(graphs.accuracy)))\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Training Error (%)')\n",
        "        plt.title('Training Error')\n",
        "\n",
        "        plt.figure(3)\n",
        "        plt.semilogy(cur_epochs, graphs.Sw_invSb)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Tr{Sw Sb^-1}')\n",
        "        plt.title('NC1: Activation Collapse')\n",
        "\n",
        "        plt.figure(4)\n",
        "        plt.plot(cur_epochs, graphs.norm_M_CoV)\n",
        "        plt.plot(cur_epochs, graphs.norm_W_CoV)\n",
        "        plt.legend(['Class Means','Classifiers'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Std/Avg of Norms')\n",
        "        plt.title('NC2: Equinorm')\n",
        "        \n",
        "        plt.figure(5)\n",
        "        plt.plot(cur_epochs, graphs.cos_M)\n",
        "        plt.plot(cur_epochs, graphs.cos_W)\n",
        "        plt.legend(['Class Means','Classifiers'])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Avg|Cos + 1/(C-1)|')\n",
        "        plt.title('NC2: Maximal Equiangularity')\n",
        "\n",
        "        plt.figure(6)\n",
        "        plt.plot(cur_epochs,graphs.W_M_dist)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('||W^T - H||^2')\n",
        "        plt.title('NC3: Self Duality')\n",
        "\n",
        "        plt.figure(7)\n",
        "        plt.plot(cur_epochs,graphs.NCC_mismatch)\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Proportion Mismatch from NCC')\n",
        "        plt.title('NC4: Convergence to NCC')\n",
        "\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}